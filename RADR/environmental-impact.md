---
description: The cost of training
---

# Environmental Impact

The computational cost of ML model training is an important consideration in environmental conservation work. Recent years have seen rapid growth in AI development, with tech companies training massive language models on internet-scale datasets requiring millions of GPU hours in specialized data centers.

### **Compute Used in This Project**

Across the 922 trained models using an NVIDIA T1200 Laptop GPU (35W TDP), this project consumed approximately **305 GPU hours** (averaging \~20 minutes per model for transfer learning with 50 epochs). At the GPU's 35W power consumption, this translates to roughly **11 kWh** of electricity.

#### **Comparative Impact**

For perspective:

* Equivalent to: Running a standard refrigerator for 3 days
* Compared to large-scale A&#x49;**:** GPT-3 training consumed \~1,287 MWh—approximately 117,000× more energy than this entire project

Applied in an intentional, focused manner, machine learning development can be done efficiently to the benefit of ecology. Targeted classifiers can achieve high performance through transfer learning with dramatically lower environmental costs. The energy investment here directly supports field conservation efforts by enabling efficient, automated monitoring of endangered species populations.
